{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "import json\n",
    "import re\n",
    "import concurrent.futures\n",
    "page_list=[page for page in range(1,4314)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crawl_url(page):\n",
    "    list_urls=[]\n",
    "    \n",
    "    data= { \"searchMode\":\"\",\n",
    "            \"searchType\":\"text\",\n",
    "            \"ExtFilter\":\"\",\n",
    "            \"sorttype\":\"1\",\n",
    "            \"keyword\":\"政治\",\n",
    "            \"rangedate\":\"[20140101 TO 20171215999:99]\",\n",
    "            \"totalpage\":\"43137\",\n",
    "            \"page\":\"%s\"%page }\n",
    "\n",
    "    resp = requests.post(\"https://tw.appledaily.com/search\",data=data)\n",
    "    soup = BeautifulSoup(resp.text,\"html5lib\")\n",
    "    select = soup.select(\"div.tbb > h2 > a\")\n",
    "    for s in select:\n",
    "        url = s.get(\"href\")\n",
    "        list_urls.append(url)\n",
    "    return list_urls\n",
    "\n",
    "list_a=[]\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=80) as executor:\n",
    "    for data in executor.map(crawl_url, page_list):\n",
    "        list_a.extend(data)     \n",
    "with open(\"appledaily_urls.txt\",\"w\") as f:\n",
    "    f.write(json.dumps(list_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"appledaily_urls.txt\",\"r\") as f:\n",
    "    r = f.read()\n",
    "urls = r.replace(\"]\",\"\").replace(\"[\",\"\").replace('\"','').split(\",\")\n",
    "\n",
    "list_a_new = [] \n",
    "for i in urls:\n",
    "    i = i.strip()\n",
    "    if i.startswith(\"https://tw.news.appledaily.com/politics/realtime\"):\n",
    "        list_a_new.append(i.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crawl_article(url):\n",
    "    list_article=[]\n",
    "    resp_co = requests.get(url)\n",
    "    soup = BeautifulSoup(resp_co.text,\"html5lib\")\n",
    "    article = {}\n",
    "    try:\n",
    "        ### datetime\n",
    "        datetime = soup.select(\"div.ndArticle_creat\")[0].text\n",
    "        article[\"datetime\"] = re.findall(\"出版時間：([2][0][1][4567]/[01][\\d]/[012][\\d])\",datetime)[0].replace(\"/\",\"-\")    \n",
    "        ### title\n",
    "        title = soup.select(\"h1\")\n",
    "        article[\"title\"] = title[0].text.replace(\"\\u3000\",\"\").replace(\"\\u200b\",\"\")\n",
    "        ### media\n",
    "        article[\"media\"]=\"appledaily\"\n",
    "        ### category\n",
    "        article[\"category\"]=\"politics\"\n",
    "        ### url\n",
    "        article[\"url\"]=url\n",
    "        ### reporter\n",
    "        article[\"reporter\"] = None\n",
    "        ### location\n",
    "        article[\"location\"] = None\n",
    "        ### content\n",
    "        article[\"content\"] = soup.select(\"div.ndArticle_margin > p\")[0].text\n",
    "        ### hash\n",
    "        article[\"hash\"] = hash(soup.select(\"div.ndArticle_margin > p\")[0].text)\n",
    "        ### id\n",
    "        article[\"id\"] = str(url.split(\"/\")[-1])\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    list_article.append(article)\n",
    "    return list_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_b=[]\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:\n",
    "    for data in executor.map(crawl_article, list_a_new):\n",
    "        print(\"finish\")\n",
    "        list_b.extend(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12257"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
