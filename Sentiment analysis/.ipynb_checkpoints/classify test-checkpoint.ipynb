{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import jieba.posseg as pseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### stopword list\n",
    "stopwords_list = [line.strip() for line in open('stopwords.txt',\"r\").readlines()]\n",
    "\n",
    "### syn dict\n",
    "syn_dict = {}\n",
    "with open(\"syn.txt\",\"r\") as f :\n",
    "    for line in f:\n",
    "        for word in line.strip(\"\\n\").split(\"\\t\")[1:]:\n",
    "            syn_dict[word] = line.strip(\"\\n\").split(\"\\t\")[0]\n",
    "\n",
    "### 中文斷詞字典\n",
    "jieba.set_dictionary(\"dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopwords(w):\n",
    "    if w not in stopwords_list:\n",
    "        return w\n",
    "\n",
    "def syn(w):\n",
    "    if w in syn_dict.keys():\n",
    "        w=syn_dict[w]\n",
    "        return w\n",
    "    else:\n",
    "        return w\n",
    "\n",
    "def cut(news):\n",
    "    w = jieba.cut(news, cut_all=False ,HMM=True)\n",
    "    return w\n",
    "\n",
    "def regular(w):\n",
    "    line = re.findall('[\\u4e00-\\u9fa5]+', w)\n",
    "    if len(line) > 0:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaning(paragraph):\n",
    "    me_words = []\n",
    "    words = cut(paragraph) \n",
    "    for w in words:\n",
    "        if len(w)>=1:\n",
    "            w = regular(w)\n",
    "            if w is not None:\n",
    "                w_stopwords = stopwords(w[0])\n",
    "                if w_stopwords is not None:\n",
    "                    w_syn = syn(w_stopwords)\n",
    "                    me_words.append(w_syn)\n",
    "    return \" \".join(me_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleaned_news(news_list):\n",
    "    cleaned_news_list = []\n",
    "    for news in copy.deepcopy(news_list):\n",
    "        if 'message' in news:\n",
    "            news[\"message\"] = text_cleaning(news[\"message\"])\n",
    "            cleaned_news_list.append(news)\n",
    "    return cleaned_news_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14712"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"sentiment.csv\",\"r\") as f:\n",
    "    r = f.read()\n",
    "\n",
    "data = [m.split(\",\")[0] for m in r.split(\"\\n\") if len(m.split(\",\")[0])!=0]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text  import  CountVectorizer  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "### X資料(message)\n",
    "vectorizer = CountVectorizer(min_df=1, token_pattern='(?u)\\\\b\\\\w+\\\\b')  \n",
    "X = vectorizer.fit_transform(data)\n",
    "X = X.toarray()\n",
    "\n",
    "## y資料(sentiment)\n",
    "y=[m.split(\",\")[1] for m in r.split(\"\\n\") if len(m.split(\",\")[0])!=0]\n",
    "y=np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf1 = MultinomialNB().fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "prediction = clf1.predict(X_train)\n",
    "print('train_Accuracy: %.2f' % accuracy_score(prediction, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "prediction_test = clf1.predict(X_test)\n",
    "print('test_Accuracy: %.2f' % accuracy_score(prediction_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n",
      "[[ 0.56869983  0.05407292  0.37722725]]\n"
     ]
    }
   ],
   "source": [
    "input_message = vectorizer.transform([\"\"\"你好\"\"\"]).toarray()\n",
    "\n",
    "print(clf1.predict(input_message))\n",
    "print(clf1.predict_proba(input_message))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Java\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler() \n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()  \n",
    "classifier.fit(X_train_std, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_Accuracy: 1.00\n",
      "test_Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = classifier.predict(X_train_std)\n",
    "pred_test = classifier.predict(X_test_std)\n",
    "print('train_Accuracy: %.2f' % accuracy_score(pred, y_train))\n",
    "print('test_Accuracy: %.2f' % accuracy_score(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.fit_transform(X_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='rbf', C=1.0, random_state=0, gamma=1)\n",
    "svm.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred = svm.predict(X_train_std)\n",
    "pred_test = svm.predict(X_test_std)\n",
    "print('train_Accuracy: %.2f' % accuracy_score(pred, y_train))\n",
    "print('test_Accuracy: %.2f' % accuracy_score(pred_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
