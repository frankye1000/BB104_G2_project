{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from pymongo import MongoClient\n",
    "import pprint\n",
    "import jieba.posseg as pseg\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Java\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.486 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "### stopwords list\n",
    "stopwords_set = set([line.strip() for line in open('stopwords.txt',\"r\").readlines()])\n",
    "\n",
    "### syn dict\n",
    "syn_dict = {}\n",
    "with open(\"syn.txt\",\"r\") as f :\n",
    "    for line in f:\n",
    "        for word in line.strip(\"\\n\").split(\"\\t\")[1:]:\n",
    "            syn_dict[word] = line.strip(\"\\n\").split(\"\\t\")[0]\n",
    "\n",
    "### 中文斷詞字典\n",
    "jieba.load_userdict(\"dict.txt\")\n",
    "# jieba.set_dictionary(\"dict.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stopwords(w):\n",
    "    if w not in stopwords_set:\n",
    "        return w\n",
    "\n",
    "def syn(w):\n",
    "    if w in syn_dict.keys():\n",
    "        w=syn_dict[w]\n",
    "        return w\n",
    "    else:\n",
    "        return w\n",
    "\n",
    "def cut(news):\n",
    "    w = jieba.cut(news, cut_all=False ,HMM=True)\n",
    "    return w\n",
    "\n",
    "def regular(w):\n",
    "    line = re.findall('[\\u4e00-\\u9fa5]+', w)\n",
    "    if len(line) > 0:\n",
    "        return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def text_noun_cleaning(text):\n",
    "    me_words = []\n",
    "    words = [(word, flag) for word, flag in pseg.cut(text)]\n",
    "    words = [word[0] for word in list(filter(lambda x: x[1] == \"n\", words))]\n",
    "    for w in words:\n",
    "        if len(w)>1:\n",
    "            if not re.match(r\"^[三|四|五|六|七|十]\", w):\n",
    "                w = regular(w)\n",
    "                if w is not None:\n",
    "                    w_stopwords = stopwords(w[0])\n",
    "                    if w_stopwords is not None:\n",
    "                        w_syn = syn(w_stopwords)\n",
    "                        me_words.append(w_syn)\n",
    "    return \" \".join(me_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# pretreat keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from  sklearn.feature_extraction.text  import  CountVectorizer\n",
    "from  sklearn.feature_extraction.text  import  TfidfTransformer  \n",
    "\n",
    "client = MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "db = client[\"<db name>\"]\n",
    "collection_names = db.collection_names()\n",
    "post_names = [collection_name for collection_name in collection_names if collection_name.endswith(\"posts\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords_list = []\n",
    "\n",
    "for post_name in post_names:\n",
    "    collection = db[post_name]\n",
    "    messages = list(collection.find({}, {\"_id\":0, \"message\":1, \"created_time\":1, \"post_id\":1, \"link\":1}))\n",
    "    \n",
    "    \n",
    "    post_list = []\n",
    "    for message in messages:\n",
    "        if \"message\" in message:\n",
    "            message[\"message\"] = text_noun_cleaning(message[\"message\"])\n",
    "            if len(message[\"message\"].split(\" \")) > 3:\n",
    "                post_list.append(message)\n",
    "                \n",
    "    message_list = [post[\"message\"] for post in post_list]\n",
    "    vectorizer = CountVectorizer()  \n",
    "    X = vectorizer.fit_transform(message_list)  \n",
    "    words = vectorizer.get_feature_names()\n",
    "    transformer = TfidfTransformer()  \n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    \n",
    "    important_word_list = []\n",
    "    for important_word in tfidf.toarray():\n",
    "        important_word_list.append(sorted(list(zip(words, important_word)), key = lambda x : x[1], reverse=True)[:10])\n",
    "        \n",
    "        \n",
    "    keywords_temp_list = []\n",
    "\n",
    "    length = len(post_list)\n",
    "    for i in range(length):\n",
    "        for word in important_word_list[i]:\n",
    "            temp = {}\n",
    "            temp[\"post_id\"] = post_list[i][\"post_id\"]\n",
    "            temp[\"created_time\"] = post_list[i][\"created_time\"]\n",
    "            temp[\"word\"] = word[0]\n",
    "            temp[\"tfidf\"] = word[1]\n",
    "            temp[\"politician_id\"] = post_list[i][\"post_id\"].split(\"_\")[0]\n",
    "            temp[\"link\"] =  post_list[i][\"link\"]\n",
    "            keywords_temp_list.append(temp)\n",
    "        print(\"finish %s\"%post_list[i][\"post_id\"])\n",
    "    keywords_list.extend(keywords_temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaned_keywords_list = [keyword for keyword in keywords_list if keyword[\"tfidf\"] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71818"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x18c016205c8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "db = client[\"<db name>\"]\n",
    "collections = db[\"<collection name>\"]\n",
    "collections.insert_many(cleaned_keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# project keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10150145806225128',\n",
       " '109391162488374',\n",
       " '118250504903757',\n",
       " '122936517768637',\n",
       " '136845026417486',\n",
       " '1380211668909443',\n",
       " '152472278103133',\n",
       " '153819538009272',\n",
       " '184799244894343',\n",
       " '191690867518507',\n",
       " '232716627404',\n",
       " '261813197541354',\n",
       " '333058400178329',\n",
       " '360151611020961',\n",
       " '365320250345879',\n",
       " '449664581882455',\n",
       " '46251501064',\n",
       " '600540963315152',\n",
       " '805460986214082',\n",
       " '852926604746233']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from  sklearn.feature_extraction.text  import  CountVectorizer\n",
    "from  sklearn.feature_extraction.text  import  TfidfTransformer  \n",
    "\n",
    "client = MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "db = client[\"<db name>\"]\n",
    "politician_id_list = db.posts.distinct(\"politician_id\")\n",
    "politician_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 10150145806225128\n",
      "finish 109391162488374\n",
      "finish 118250504903757\n",
      "finish 122936517768637\n",
      "finish 136845026417486\n",
      "finish 1380211668909443\n",
      "finish 152472278103133\n",
      "finish 153819538009272\n",
      "finish 184799244894343\n",
      "finish 191690867518507\n",
      "finish 232716627404\n",
      "finish 261813197541354\n",
      "finish 333058400178329\n",
      "finish 360151611020961\n",
      "finish 365320250345879\n",
      "finish 449664581882455\n",
      "finish 46251501064\n",
      "finish 600540963315152\n",
      "finish 805460986214082\n",
      "finish 852926604746233\n"
     ]
    }
   ],
   "source": [
    "collection = db[\"posts\"]\n",
    "\n",
    "keywords_list = []\n",
    "\n",
    "for politician_id in politician_id_list:\n",
    "    \n",
    "    messages = list(collection.find({\"politician_id\": politician_id}, \n",
    "                                    {\"_id\":0, \"message\":1, \"created_time\":1, \"post_id\":1, \"shares\":1, \"likes\":1, \"comment_numbers\":1, \"post_score\":1,\"link\":1}))\n",
    "    \n",
    "    \n",
    "    post_list = []\n",
    "    for message in messages:\n",
    "        if \"message\" in message:\n",
    "            message[\"message\"] = text_noun_cleaning(message[\"message\"])\n",
    "            if len(message[\"message\"].split(\" \")) > 3:\n",
    "                post_list.append(message)\n",
    "    message_list = [post[\"message\"] for post in post_list]\n",
    "    vectorizer = CountVectorizer()  \n",
    "    X = vectorizer.fit_transform(message_list)  \n",
    "    words = vectorizer.get_feature_names()\n",
    "    transformer = TfidfTransformer()  \n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    \n",
    "    important_word_list = []\n",
    "    for important_word in tfidf.toarray():\n",
    "        important_word_list.append(sorted(list(zip(words, important_word)), key = lambda x : x[1], reverse=True)[:10])\n",
    "        \n",
    "        \n",
    "    keywords_temp_list = []\n",
    "\n",
    "    length = len(post_list)\n",
    "    for i in range(length):\n",
    "        for word in important_word_list[i]:\n",
    "            if word[1] != 0:\n",
    "                temp = {}\n",
    "                temp[\"post_id\"] = post_list[i][\"post_id\"]\n",
    "                temp[\"created_time\"] = post_list[i][\"created_time\"]\n",
    "                temp[\"word\"] = word[0]\n",
    "                temp[\"tfidf\"] = word[1]\n",
    "                temp[\"politician_id\"] = post_list[i][\"post_id\"].split(\"_\")[0]\n",
    "                temp[\"comment_numbers\"] = post_list[i][\"comment_numbers\"]\n",
    "                temp[\"likes\"] = post_list[i][\"likes\"]\n",
    "                temp[\"shares\"] = post_list[i][\"shares\"]\n",
    "                temp[\"post_score\"] = post_list[i][\"post_score\"]\n",
    "                if \"link\" in post_list[i]:\n",
    "                    temp[\"link\"] = post_list[i][\"link\"]\n",
    "                else:\n",
    "                    temp[\"link\"] = None\n",
    "                temp[\"new_tfidf\"] = temp[\"tfidf\"] * (1 + math.log( 1+ temp[\"likes\"]/10000, math.e)) * (1 + math.log( 1+ temp[\"shares\"]/100, math.e)) * (1 + math.log( 1+ temp[\"comment_numbers\"]/100, math.e))\n",
    "                keywords_temp_list.append(temp)\n",
    "    print(\"finish %s\"%politician_id)\n",
    "    keywords_list.extend(keywords_temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleaned_keywords_list = [keyword for keyword in keywords_list if keyword[\"tfidf\"] != 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1d5d402bb48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(\"mongodb://127.0.0.1:27017\")\n",
    "db = client[\"<db name>\"]\n",
    "collections = db[\"<collection name>\"]\n",
    "collections.insert_many(keywords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
